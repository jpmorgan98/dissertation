\chapter{Monte Carlo Methods}

\epigraphhead[10]{\singlespacing
    \epigraph{
        There's a point, around the age of twenty, when you have to choose whether to be like everybody else the rest of your life, or to make a virtue of your peculiarities.
    }
    {Ursula K. Le Guin}
}

\label{chap:mc_methods_intro}


This chapter provides specific background and introduction to Monte Carlo neutron radiation transport methods.
I describe algorithms of the method itself as well as how it has previously been implemented on GPUs in production codes.
I further motivate and introduce the concepts of portability frameworks and describe the basics of the software engineering design implemented in a new Monte Carlo neutron transport code Monte Carlo / Dynamic Code (MC/DC).
MC/DC is purpose built to rapidly deploy new numerical methods for time dependent radiation transport on modern heterogeneous HPCs.

The birth of the modern Monte Carlo method has the traditional flair of mid-century atomic science.
Legend has it Monte Carlo was born of a game of solitaire Stanisław Ulam was playing \cite{metropolis_1987_history}.
Earlier that year, Ulam and John von Neumann\footnote{This is the second time contributions from John von Neumann have been used in basic background of this dissertation.}
had been observing the testing of the first general programmable computer: the ENIAC machine, where it was being built at the University of Pennsylvania in Maryland \cite{anderson_1986_maniac}\footnote{Subrahmanyan Chandrasekhar who derived the method of discrete ordinance (discussed in chapter \ref{chap:determ_intro}) was also at the Aberdeen proving ground working on hydrodynamic calculations for ballistic projectiles at the same time as von Neumann and Ulam. I do not know if they ever met however the first published application of discrete ordinance to neutron transport was not until 1953 by Bregt Carlson \cite{carlson_1955_sn, carlson_1958_sn}.
}.
The idea popped into his head: what if I could play hundreds of games of solitaire simultaneously using ENIAC to determine the best outcome.
From there von Neumann and Ulam formalized the ideas of what would become known as Monte Carlo in their famous March 1947 letter to Robert Richtmeyer\footnote{
Robert Richtmeyer of the Lax–Richtmyer equivalence theorem \cite{Lax_1956_stability}.
}.
Nicolas Metropolis was shortly brought on board and suggested the codename ``Monte Carlo" for the method of method statistical sampling.
The legend continuous that Ulam appreciated this connection to the European casinos as his paternal uncle, Michał Ulam, was a notorious gambler who frequented the city-state of Monaco and its jurisdiction---Monte Carlo \cite{anderson_1986_maniac}.

While ENIAC was being moved from where it was built at the University of Pennsylvania in Philadelphia to the Aberdeen Proving Grounds in Maryland (a process which took two years), Enrico Fermi developed a tool to aid in the analog computation of Monte Carlo solutions:
Monte Carlo trolleys or \textit{Fermiacs} would physically trace a neutron's random walk on a paper where a phase space was actually drawn out \cite{metropolis_1987_history}.
From there tallies could manually accumulated and quantities of interest determined.
When ENIAC was finally turned on again in 1950. Monte Carlo was the first stress test of the machine and performed calculations for the super or hydrogen bomb \cite{metropolis_1987_history}.

In 1948 while ENIAC was still in Philadelphia, Metropolis along with Klára Dán von Neumann\footnote{
Klára Dán von Neumann was not only the wife of John von Neumann and a mother of modern programming, but she was also a Hungarian national champion in figure skating \cite{dyson_2012_digital}.}
and others\footnote{Foster and Cerda Evans, Harris and Rosalie Mayer, and Marshall Rosenbluth \cite{switman_2017_unheralded}.} converted ENIAC into a machine with a stored program, effectively beginning modern computing as we understand it today.
The first unclassified publication on the Monte Carlo method was in 1949 \cite{metropolis_1949_monteCarlo}.

Like most profound numerical schemes, the method statistical sampling was probably discovered by multiple people across history---reports claim that Enrico Fermi was using the method for neutrons as early as 1939.
But it was not until 1947 that nescient existence of computational power dovetailed with a game of solitaire to create the Monte Carlo method of today.
The Monte Carlo method was born alongside the computer and seemingly major components of the computer were forged by the need for Monte Carlo calculations.

Monte Carlo methods have expanded as a wholly significant subfield of numerical analysis next to the more classically derived deterministic methods---or as Metropolis called them ``the method of differential equations"- \cite{anderson_1986_maniac}.
Monte Carlo has been applied to countless types physics.
Monte Carlo neutronics specifically has stayed at the bleeding edge of both the development of the numerical method as well as the computers to calculate solution.
This dissertation humbly attempts to make a contribution to this rich and lively field.


\section{Monte Carlo Algorithms}

Conducting a direct simulation Monte Carlo numerical experiment requires two basic algorithmic components.
The first is some way of sampling discontinuous\footnote{Discontinuities are due to differing materials.} cumulative probability distribution functions in an unbiased way (such as surface (\ref{c5:surface_tracking}) and Woodcock-delta (\ref{c5:delta_tracking}) tracking algorithms).
The second is a method of estimating quantities of interest from events that are happening as they undergo the Monte Carlo process (track length collision estimators (\ref{c5:estimators}).

\subsection{Surface Tracking}
\label{c5:surface_tracking}

Surface tracking is often used track the movement of neutrons within a system. 
For a particle in material $m$, a distance to collision is sample from a cumulative probability distribution function by
\begin{equation}
    d_{\text{collision}} = \frac{-\ln(\xi)}{\Sigma_{t,m}(E)} \; ,
\end{equation}
where $\Sigma_{t,m}(E)$ [\SI{}{\per\centi\meter}] is the macroscopic total cross section at a given energy $E$ of the $m$ material and $\xi$ is a pseudo-random number between zero and one.

This sampling of the cumulative probability distribution function will only hold true while the material is homogeneous.
If the distance to collision is beyond a material interface in a system with multiple materials, the particle must be stopped at that interface surface and a new distance to collision must be calculated with with the new material's $\Sigma_{t,m}$.
This approach is an unbiased way of dealing with the sampling of distance to collision in a heterogeneous medium.
In a standard surface tracking algorithm this involves computing both a distance to collision ($d_{\text{collision}}$) and a distance to the nearest surface along the particle's direction of travel ($d_{\text{surface}}$).
The smaller of these two distances determines which event happens to the particle: a collision or a surface crossing.
After or while the particle is moving, tallies can be accumulated to compute quantities of interest.
If a collision occurs, more sampling and operations can be done (e.g., isotropically scatter a particle).
If the particle is still alive at the end, the algorithm is repeated.
The distance to nearest surface computation can become quite expensive as geometries grow in complexity (e.g., complex cumulative solid geometries or CAD based surfaces).
Surface tracking is at the heart of many modern Monte Carlo neutron transport applications, including MCNP \cite{MCNP_RisingArmstrongEtAl}, Shift \cite{hamilton_continuous-energy_2019, pandya_implementation_2016}, MONK/MCBEND \cite{richards_monk_2015}, and OpenMC \cite{romano_openmc_2015}.

\subsection{Delta tracking}
\label{c5:delta_tracking}

Delta tracking is the next most common tracking approach and is shown in Algorithm \ref{alg:trad}.
It starts by pre-processing a \textit{majorant} macroscopic cross-section
\begin{equation}
    \maj(E) = \max\left({\Sigma_{t,1}(E), \Sigma_{t,2}(E) \dots \Sigma_{t,m}(E) \dots \Sigma_{t,M}(E)}\right)
\end{equation}
such that it is the largest cross section at any given point in any material in the problem.
Figure \ref{fig:majorant_c5ce} shows a macroscopic majorant cross section for a typical pressurized water reactor.
When delta tracking the distance to collision is always sampled with the majorant,
\begin{equation}
    d_{\text{collision}} = \frac{-\ln{\xi}}{\maj(E)} \; ,
\end{equation}
ignoring surface crossing events.
However, this sampling forces extra collisions that do not physically exist; using the majorant generates the smallest distance to collision.
Delta tracking algorithms use a rejection sampling to determine if the sampled collision event was \textit{real} or a \textit{virtual} (\textit{phantom}) collision.
At the particle's current location, the true material region is identified to compute $\Sigma_{t,m}$.
If 
\begin{equation}
    \xi > \frac{\Sigma_{t,m}(E)}{\maj(E)} \; ,
\end{equation}
where $\xi$ is a new random number, the collision did not physically occur, is rejected, and the particle can be left alive on its current direction of travel and energy.
From here the process is the same as before: tallies are accumulated, real collision physics are carried out when appropriate, and the algorithm continues as long as the particle is still alive.

An added complication when using the Woodcock-delta tracking method is restricting what type of tallies can be efficiently scored.
There are many so called estimators that can be used to indicate quantities of interest (often scalar flux) by tallying events that occur within a given region of phase space.
The two common estimators are the collision estimator and the the track (or path) length estimator.

Delta tracking algorithms are implemented in many production Monte Carlo neutron transport applications including Serpent \cite{leppanen_2010_burnup, leppanen_use_2017, leppanen_development_2013}, MONK/MCBEND \cite{richards_monk_2015}, IMPC \cite{fang_development_2022}, and GUARDYAN \cite{molnar_gpu_based_2019}.
Notably, the MONK Monte Carlo neutron transport code is the direct successor to the GEM code where Woodcock \textit{et al.} first implemented Woodcock-delta tracking \cite{woodcock_techniques_1965}.
There are other weighted Woodcock-delta tracking algorithms; in this work we explore variants of the non-weighted version \cite{molnar_variance_2018, morgan_weighted-delta-tracking_2015}.

\subsection{Estimators}
\label{c5:estimators}

Two common estimators for evaluating quantities of interest are the collision estimator and the the track (or path) length estimator.
Often tallies are scored to bins on a structured mesh grid overlying the surfaces and material regions that a Monte Carlo simulation uses to conduct actual transport operations.
The track-length estimator is
\begin{equation}
    \hat{\phi}_n = \sum_{i=1}^{I}p_{i,n} \; ,
\end{equation}
where $\hat{\phi}_n$ is the integrated scalar flux over given mesh cell $p_i$ is the track-length of particle $i$ passing through mesh cell $n$.
The collision estimator is,
\begin{equation}
    \hat{\phi}_n = \sum_{i=1}^{I} \frac{1}{\Sigma_{t,n}(E)} \;.
\end{equation}
The collision estimator will often produce a more variant solution as compared to other estimators for tallies where $\Sigma_{t,m}$ is small (optically thin, less dense materials) and will never tally anything into a void region.
On the other hand, the track-length estimator will always tally into every mesh bin as a particle moves.
Meaning, under most problem regimes, more information will be scored, which will result in a lower-variant tally for the same number of particles.
Tallies from the same particle can be combined to get an even better estimate of the quantities of; interest, however then covariance must be considered \cite{urbatsch_estimation_1995}.


\section{Monte Carlo on GPUs}

Regardless of tracking method, how best to implement Monte Carlo neutron transport on heterogeneous HPC architectures (with GPUs) is under active research and development.
We examine software engineering approaches in three Monte Carlo neutron transport codes in depth: Shift, Mercury, and OpenMC.

Shift is a Monte Carlo neutron transport package available as part of the SCALE library from Oak Ridge National Laboratory \cite{pandya_implementation_2016}.
Shift allows for multi-group and continuous-energy transport on Nvidia and AMD GPUs.
On GPUs Shift uses both event-based transport algorithms written in C with Cmake macros alternating language divergences from one vendor to the other, with Nvidia compiling with NVCC and AMD with HIP compilers \cite{mcsummit}.
The Shift development team has reported that, for multi-group GPU algorithms, all GPUs on a given node allow for 7--80 $\times$ speedup compared to all CPUs of those same nodes for a set of C5G7-type problems \cite{hamilton_multigroup_2018}.

Similarly, GPU support for the Mercury Monte Carlo neutron transport solver from Lawrence Livermore National Laboratory is enabled by an event-based algorithm written in C-CUDA \cite{pozulp_progress_2023, pozulp_sna_2024}.
For a Godiva-in-water continuous-energy benchmark problem, Mercury has shown up to a 7.7 times speedup on a whole node of the Sierra class systems as compared to a whole node of Intel Xeon dual-socket type CPUs \cite{pozulp_progress_2023}.
The Mercury development team has announced initial development efforts porting to AMD MI300A APUs \cite{pozulp_sna_2024}.
Other GPU codes have been developed as standalone C-CUDA projects directly targeting GPUs, including PRAGMA \cite{choi_optimization_2021}, GUARDYAN \cite{molnar_gpu_based_2019}, and a CUDA-based port of OpenMC \cite{ridley2021}.

%% OpenMC
OpenMC from Argonne National Laboratory differs from these approaches.
OpenMC uses the OpenMP target-offloading model to compile and execute on Nvidia, AMD, and Intel GPUs for event-based transport algorithms.
OpenMC is currently the only Monte Carlo neutron transport code enabled to run on Intel's PVC GPUs.
Furthermore, the OpenMC team has demonstrated node scaling on Nvidia, AMD, and Intel GPUs.
The OpenMC development team reports that, for the continuous-energy exaSMR test problem, they achieve a 70 times speed up on Intel GPUs as compared to a dual-socket Intel Xeon CPU node \cite{tramm2024performanceportablemontecarlo}.


\section{Portability Frameworks for Monte Carlo methods}

In this section I introduce initial investigations into high-level performance portability frameworks.
Developing software to simulate physical problems that demand HPC is difficult.
Modern HPCs commonly use both CPUs and GPUs from various vendors.
Years can be spent porting a code from CPUs to run on GPUs, then again when moving from one GPU vendor to the next \cite{pozulp_progress_2023}.
Portability issues compound when designing software for rapidly developing numerical methods where algorithms need to be both implemented and tested at scale.
Finding a software engineering approach that balances the need for portability, rapid development, open collaboration, and performance can be challenging especially when numerical schemes do not rely on operations commonly implemented in libraries  (i.e., linear algebra as in LAPACK or Intel MKL). 

Common HPC software engineering requirements are often met using a Python-as-glue-based approach, where peripheral functionality (e.g., MPI calls, I/O) is implemented using Python packages but compiled functions are called through Python's C interface where performance is needed.
Python-as-glue does not necessarily assist in producing the compiled compute kernels themselves---what the Python code is gluing together---but can go a long way in simplifying the overhead of peripheral requirements of HPC software.
With this technique, environment management and packaging uses \texttt{pip}, \texttt{conda}, or \texttt{spack}, input/output with \texttt{h5py}, MPI calls with \texttt{mpi4py}, 
and automated testing with \texttt{pytest}, which can all ease initial development and continued support for these imperative operations. 

Many tools have been developed to extend the Python-as-glue scheme to allow producing single-source compute kernels for both CPUs and GPUs.
% a DSL, pyfr
One tactic is to use a domain-specific language to avoid needing a low-level language (e.g., FORTRAN, C).
A domain-specific language is designed to alleviate development difficulties for a group of subject-area experts and can abstract hardware targets if defined with that goal.
%It can even abstract hardware targets if it is defined with that goal.
PyFR, for example, is an open-source computational fluid dynamics solver that implements a domain-specific language plus Python structure to run on CPUs and Nvidia, Intel, and AMD GPUs~\cite{pyfrPetascale}. 
%The overhead of this Python glue is less than 1\% in PyFR.
Witherden et al.~\cite{pyfrPetascale} discussed how this scheme allows PyFR developers to rapidly deploy numerical methods at deployment HPC scales and have demonstrated performance at the petascale.

Other projects have addressed the need to write user-defined compute kernels entirely in Python script.
Numba is a compiler that lowers a small subset of Python code with NumPy arrays and functions into LLVM, then just in time (JIT) compiles to a specific hardware target \cite{lam_numba_2015}. 
Numba also compiles global and device functions for Nvidia GPUs from compute kernels defined in Python.
API calls are made through Numba on both the Python side (e.g., allocate and move data to and from the GPU) and within compiled device functions (e.g., to execute atomic operations).
When compiling to GPUs, Numba supports an even smaller subset of Python, losing most of the operability with NumPy functions.
If functions are defined using only that smallest subset, Numba can compile the same functions to CPUs or GPUs, or execute those functions purely in Python.
Numba data allocations on the GPU can be consumed and ingested by functions from CuPy if linear-algebra operations are required in conjunction with user-defined compute kernels.
When targeting use of a Python portability scheme to HPC for neutron transport I compared the same transient Monte Carlo neutron transport algorithm in various implementations \cite{morgan2022} using PyKokkos \cite{AlAwarETAL21PyKokkos}, PyCUDA/PyOpenCL \cite{kloeckner_pycuda_2012}, and Numba \cite{lam_numba_2015}.
After these initial investigations a Numba+mpi4py software engineering scheme was deemed the most viable for implementation into MC/DC.

\section{Summary and Relation to Research Questions}


Chapter \ref{chap:joss_paper} briefly descries the software engineering layout in MC/DC. MC/DC avoids the need for a compiled or domain-specific language by using the Numba compiler for Python to accelerate and abstract our compute kernels to near compiled-code speeds.
Chapter \ref{chap:joss_paper} describes novel algorithms in MC/DC.
This chapter provides more data on \emph{RQ1}.

Chapter \ref{chap:cise_paper} explains MC/DC's software engineering approach in greater depth and discusses how it allows for portability, rapid development, and open collaboration for high-performance computing on GPUs and CPUs. 
It discusses a portability scheme using the Numba compiler for Python in Monte Carlo / Dynamic Code (MC/DC), a new neutron transport application for rapidly developing Monte Carlo. 
Using this scheme, MC/DC can run as a pure Python, compiled CPU, or compiled GPU solver. 
Chapter \ref{chap:cise_paper} presents performance results (including weak scaling up to 256 nodes) for a time-dependent problem on both CPUs and GPUs comparing favorably to a production C++ code.
This work is supplemented by Appendix \ref{chapter:mcdc_prof} which provides greater depth on the bespoke just-in-time compilation structure MC/DC uses to compile to GPUs.
This chapter further evaluates \emph{RQ1}.

Chapter \ref{chap:delta_tracking_paper} leads with the theory that there is no mathematical reason why a track-length estimator cannot be used in conjunction with Woodcock-delta tracking---only implementation issues. 
In this work we take advantage of that idea to produce a Woodcock-delta tracking algorithm, which tallies fluxes to a structured rectilinear mesh using the track-length estimator.
This development readily enables hybrid surface-delta tracking algorithms as the track-length tally can be used everywhere for scalar flux estimation regardless of which tracking algorithm a particle is using.
Furthermore, chapter \ref{chap:delta_tracking_paper} discusses a novel hybrid-in-energy method, where Woodcock-delta tracking is used in high energies (where mean free paths are long) and surface tracking below that (starting at the neutron resonances) as well as a previously defined hybrid-in-material method.
This chapter also verifies that delta tracking algorithms we consider can be used in conjunction with continuously moving surfaces.
The solution and performance of four time-dependent benchmark problems demonstrates the comparative performance of the new and traditional methods implemented in this work.
Woodcock-delta tracking with a track-length tally shows modest improvements to figures of merit as compared to traditional delta tracking with a collision estimator and surface tracking with a track-length estimator (\num{1.5}$\times$--\num{2.5}$\times$) and significant improvements (\num{7}$\times$--\num{11}$\times$) when using the hybrid-in-energy method.
This work is supplemented by Appendix \ref{chapter:mcatk_paper} and further provides evidence for \emph{RQ1} while also indicating conclusions for \emph{RQ5}.