

Implementing the OCI and SI approaches on GPUs requires a numerical linear algebra solver library like LAPACK~\cite{laug}.
Many high-performance open-source linear algebra tools exist (e.g., Trillinos, PETSc, SUNDIALS, MAGMA), but we chose a vendor-supplied package depending on the hardware target of choice.
Our target hardware is an AMD MI250X so we use the AMD ROCm compute library to solve the system of equations.
Modern GPU vendor-supplied LAPACK libraries often include a \texttt{batched} class of solvers,
which operate on a group of like-sized systems in unison and are optimized by the hardware vendors.
For example, LU decomposition with pivoting (a generic direct solver for a system of linear equations) used in this work comes from RocSolver's \texttt{strided\_batched\_dgesv} \cite{rocsolver}.

We use direct solvers here because all systems are ``small", with orders ranging between 4 and 100.
This makes the use of a batched implementation of LU decomposition with pivoting ideal.
Furthermore LAPACK-type implementations of \texttt{\_gesv} automatically return the $L+U+D$ decomposition of $A_j$.
So, in subsequent iterations, this system can be back solved quickly (using LAPACK \texttt{\_getrs}).
%For SI we found that these kinds of optimizations are not as impactful to overall runtime, as SI often requires fewer iterations to converge a given problem.
In this mode for both SI and OCI, the only user-defined device kernels are the RHS vector builders which are already memory safe operations.

This software engineering design will increase the memory footprint of OCI and SI as the $A_j$ matrices are stored in memory.
This is acceptable for 1D transport but more optimization may be required when moving to 2D and 3D solvers.
%User-defined kernels were profiled to ensure they did not involve onerous data transport.

Algorithm \ref{alg:si} describes the convergence loop for source iteration.
In this case the $A$ and $b$ matrices are of dimension four.
The number of systems to solve changes with the number of angles ($N$), groups ($G$), and cells ($J$).
SI requires host-side dispatching in every cell to execute the sequential nature of the sweep.
This algorithm has been implemented such that all available computing is done at once (negative sweeps are happening in unison with positive ones in all angles and groups).
Group-to-group communication is done at the end of every iteration.
%The system and solution matrices are stored such that only two vectors are ever kept of the PBJ systems.
The first iteration calls the full \texttt{\_gesv} algorithm, which returns the solution of the system and the $L+U+D$ decomposition in $A$.
Subsequent iterations just perform a back substitution (\texttt{\_getrs}) .
Profiling shows that host functions (including host$\rightarrow$device and device$\rightarrow$host communication) account for up to around $9\%$ of the runtime in the largest problems we considered.
%As the RHS is changing significantly in every iteration that it is built on the CPU and moved back to the GPU between iterations.

\SetKwComment{Comment}{//}{ }
\DontPrintSemicolon
\begin{algorithm}
    \label{alg:si}
    build $A$ in all cells and move to Device

    $\beta$ = $4NG$ \Comment*[r]{offset to a cell} %\Comment*[r]{offset to a cell}

    $l = 0$ \Comment*[r]{iteration counter}

    converged = false

    \While{!converged}{

    build constant part of $b$ in all cells

    move constant part of $b$ to Device
        
        \For{j = 0 to $J$ \Comment*[r]{transport sweep}}{ 

            build variable part of $b$ at cells $j$ and $J-i$ 
            
            \If{l=0}{
                \Comment*[r]{in Aj out L+U+D}
                $\Psi_j$ = \texttt{GPU\_strided\_batched\_dgesv}($A$[$\beta^{2}j$],$b$[$\beta j$])
            }\Else{
                \Comment*[r]{back substitution}
                $\Psi_j$ = \texttt{GPU\_strided\_batched\_dgetrs}($A$[$\beta^2j$],$b$[$\beta j$]) 
            }
            }

            move $\Psi$ to Host

            $\Phi^l_j = \sum_{n=0}^{N} w _n\Psi^{l}_{j,n}$

            $e=||\Phi^l - \Phi^{l-1}||_2$

            $\rho_e = e^l / e^{l-1}$

            \If{$e < \epsilon(1-\rho_e)$}{
                converged = true
            }

            $e^{l-1} = e^l$ 

            $l++$
        
        move $\Phi^l$ to Host
        
        communicate group to group sources
    }
    \caption{Source iteration algorithm implemented on GPU. Simplified for brevity.}
\end{algorithm}
%GPU side optimizations where not as important for source iterations as it uses so many fewer itterations to converge the system in 

Algorithm \ref{alg:ocigpu} describes OCI's on-GPU convergence loop.
We found OCI to be more sensitive to within-iteration optimizations.
In some cases (specifically in the thin limit) OCI may require significantly more iterations to converge.
For that reason, it is imperative that the OCI iteration take place entirely on the GPU.
Luckily, OCI's algorithm is simpler to implement on GPUs because group-to-group communication happens within the solved systems.
We implemented the following algorithm to do that: everything under the \texttt{while} loop is wholly contained on the GPU, requiring minimal device-to-host communication.

\begin{algorithm}
    build $A$ in all cells and move to device

    build constant part of $b$ in all cells and move to device

    $l = 0$ \Comment*[r]{iteration counter}

    converged = false

    \While{!converged}{
        \If{l=0}{
            \Comment*[r]{A in-out becomes the L+U+D decomp}
            \Comment*[r]{b in-out becomes the solution vector}
            $\Psi$ = \texttt{GPU\_strided\_batched\_dgesv}($A$,$b$)
        }\Else{
            \Comment*[r]{back substitution}
            $\Psi$ = \texttt{GPU\_strided\_batched\_dgetrs}($A$,$b$) 
        }

        $e=||\Psi^l - \Psi^{l-1}||_2$ \Comment*[r]{Done on GPU using rocBLAS dr2n}

        $\rho_e = e^l / e^{l-1}$ \Comment*[r]{spectral radius estimation}

        \If{$e < \epsilon(1-\rho_e)$\Comment*[r]{controlling for false convergence}} { 
            converged = true
        }

        $e^{l-1} = e^l$

        $b^{l-1} = b^l$

        $l++$
            
    }
    
    move $\Psi$ to host
    
    \caption{One-cell inversion algorithm implemented on GPUs. Simplified for brevity.}
    \label{alg:ocigpu}
\end{algorithm}

OCI's systems are represented as dense within a cell and built in a strided-batched configuration to take advantage of the block sparsity.
However, now systems within an iteration can be dispatched in unison.
%This OCI algorithm allows the solvers provided from RocSolver decide on the most advantageous parallelism structure effectively off loading.
%The use makes no decisions about thread blocks.
The intra-iteration $b$-vector production kernels are the only user-defined device functions required in this algorithm. These are relatively simple to implement as they are thread-safe operations.

%In higher dimensions this algorithm could remain the same and still be performant, again something that cannot be done traditional source iterations which requires KBA.
%In production codes the A systems are usually built on the fly as to avoid filling memory.
%This wouldn't necessarily preclude the use of these strided batch
%This work did not do that as
%One could imagine an algorithm that 
%An additional consideration would have to be taken if an acceleration scheme is used in conjunction with this iterative scheme 